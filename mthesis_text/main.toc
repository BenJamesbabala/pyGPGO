\contentsline {chapter}{\numberline {1}Organization of this work}{5}{chapter.1}
\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}
\contentsline {section}{\numberline {1.2}Organization of the thesis}{6}{section.1.2}
\contentsline {chapter}{\numberline {2}Gaussian Process regression}{7}{chapter.2}
\contentsline {section}{\numberline {2.1}A function space view for Gaussian Processes}{7}{section.2.1}
\contentsline {section}{\numberline {2.2}A weight space view for Gaussian Processes}{9}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Standard Bayesian linear regression}{9}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Kernel functions in feature space}{9}{subsection.2.2.2}
\contentsline {section}{\numberline {2.3}Prediction using a Gaussian Process prior}{10}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}A toy example of Gaussian Process regression}{12}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Picking a winner}{12}{subsection.2.3.2}
\contentsline {section}{\numberline {2.4}On covariance functions}{13}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Visualizing different covariance functions}{14}{subsection.2.4.1}
\contentsline {section}{\numberline {2.5}Hyperparameter optimization}{14}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Type II Maximum Likelihood}{14}{subsection.2.5.1}
\contentsline {subsubsection}{Another toy example: Optimizing the characteristic length-scale}{16}{section*.5}
\contentsline {subsection}{\numberline {2.5.2}Cross validation}{16}{subsection.2.5.2}
\contentsline {section}{\numberline {2.6}Further theoretical aspects}{17}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Gaussian processes as linear smoothers}{17}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Explicit basis functions}{18}{subsection.2.6.2}
\contentsline {chapter}{\numberline {3}Bayesian optimization}{21}{chapter.3}
\contentsline {section}{\numberline {3.1}Preliminaries}{21}{section.3.1}
\contentsline {section}{\numberline {3.2}The bayesian optimization framework}{21}{section.3.2}
\contentsline {section}{\numberline {3.3}On acquisition functions}{22}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Improvement-based policies}{22}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Optimistic policies}{23}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}Information-based policies}{23}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}Acquisition function portfolios}{24}{subsection.3.3.4}
\contentsline {subsection}{\numberline {3.3.5}Visualizing the behaviour of an acquisition function}{24}{subsection.3.3.5}
\contentsline {subsection}{\numberline {3.3.6}Why does Bayesian Optimization work?}{24}{subsection.3.3.6}
\contentsline {section}{\numberline {3.4}Role of GP hyperparameters in optimization}{26}{section.3.4}
\contentsline {section}{\numberline {3.5}Optimizing the acquisition function}{27}{section.3.5}
\contentsline {section}{\numberline {3.6}Computational costs}{28}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Approximations to the analytical GP. Alternative surrogates.}{28}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Parallelization}{29}{subsection.3.6.2}
\contentsline {section}{\numberline {3.7}Step-by-step examples}{30}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Optimizing the sine function}{30}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Optimizing the Rastrigin function}{30}{subsection.3.7.2}
\contentsline {chapter}{\numberline {4}Experiments}{35}{chapter.4}
\contentsline {section}{\numberline {4.1}Benchmarking rules}{35}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Other strategies for hyper-parameter optimization}{35}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Evaluation metrics}{36}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Bayesian optimization setup}{37}{subsection.4.1.3}
\contentsline {subsection}{\numberline {4.1.4}Machine-learning models used}{37}{subsection.4.1.4}
\contentsline {section}{\numberline {4.2}The binding affinity dataset}{37}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Description of the problem}{37}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Description of the dataset}{38}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Experiments}{39}{subsection.4.2.3}
\contentsline {section}{\numberline {4.3}The protein-protein interface prediction dataset}{42}{section.4.3}
\contentsline {chapter}{\numberline {5}pyGPGO: A simple Python Package for Bayesian Optimization}{43}{chapter.5}
\contentsline {section}{\numberline {5.1}Installation}{43}{section.5.1}
\contentsline {section}{\numberline {5.2}Usage}{44}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}A minimal example}{44}{subsection.5.2.1}
\contentsline {section}{\numberline {5.3}Examples}{46}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Gaussian Process regression using the \texttt {GPRegressor} module.}{46}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Optimizing parameters of a machine-learning model using the \texttt {GPGO} module.}{47}{subsection.5.3.2}
\contentsline {section}{\numberline {5.4}Features}{48}{section.5.4}
\contentsline {section}{\numberline {5.5}Comparison with existing software}{48}{section.5.5}
\contentsline {section}{\numberline {5.6}Future work}{48}{section.5.6}
