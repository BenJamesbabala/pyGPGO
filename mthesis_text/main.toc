\contentsline {chapter}{\numberline {1}Organization of this work}{5}{chapter.1}
\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}
\contentsline {section}{\numberline {1.2}How do I read this?}{6}{section.1.2}
\contentsline {chapter}{\numberline {2}Gaussian Process regression}{7}{chapter.2}
\contentsline {section}{\numberline {2.1}A weight space view for Gaussian Processes}{7}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Standard Bayesian linear regression}{7}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Kernel functions in feature space}{8}{subsection.2.1.2}
\contentsline {section}{\numberline {2.2}A function space view for Gaussian Processes}{8}{section.2.2}
\contentsline {section}{\numberline {2.3}Prediction using a Gaussian Process prior}{11}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}A toy example of Gaussian Process regression}{13}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Picking a winner}{14}{subsection.2.3.2}
\contentsline {section}{\numberline {2.4}On covariance functions}{16}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}A zoo of covariance functions}{18}{subsection.2.4.1}
\contentsline {section}{\numberline {2.5}Hyperparameter optimization}{20}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Type II Maximum Likelihood}{20}{subsection.2.5.1}
\contentsline {subsubsection}{Another toy example: Optimizing the characteristic length-scale}{21}{section*.5}
\contentsline {subsection}{\numberline {2.5.2}Cross validation}{22}{subsection.2.5.2}
\contentsline {section}{\numberline {2.6}Further theoretical aspects}{22}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Gaussian processes as linear smoothers}{22}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Explicit basis functions}{23}{subsection.2.6.2}
\contentsline {chapter}{\numberline {3}Bayesian optimization}{25}{chapter.3}
\contentsline {section}{\numberline {3.1}Preliminaries}{25}{section.3.1}
\contentsline {section}{\numberline {3.2}The bayesian optimization framework}{25}{section.3.2}
\contentsline {section}{\numberline {3.3}On acquisition functions}{26}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Improvement-based policies}{26}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Optimistic policies}{27}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}Information-based policies}{27}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}Acquisition function portfolios}{28}{subsection.3.3.4}
\contentsline {subsection}{\numberline {3.3.5}Visualizing the behaviour of an acquisition function}{28}{subsection.3.3.5}
\contentsline {subsection}{\numberline {3.3.6}Why does Bayesian Optimization work?}{29}{subsection.3.3.6}
\contentsline {section}{\numberline {3.4}Role of GP hyperparameters in optimization}{32}{section.3.4}
\contentsline {section}{\numberline {3.5}Optimizing the acquisition function}{32}{section.3.5}
\contentsline {section}{\numberline {3.6}Computational costs}{33}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Approximations to the analytical GP. Alternative surrogates.}{33}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Parallelization}{34}{subsection.3.6.2}
\contentsline {section}{\numberline {3.7}Step-by-step examples}{35}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Optimizing (yet-again) the sine function}{35}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Optimizing the Rossenbrock function}{35}{subsection.3.7.2}
\contentsline {chapter}{\numberline {4}Experiments}{39}{chapter.4}
\contentsline {chapter}{\numberline {5}pyGPGO: A simple Python Package for Bayesian Optimization}{41}{chapter.5}
