\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Organization of this work}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}How do I read this?}{6}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Gaussian Process regression}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A weight space view for Gaussian Processes}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Standard Bayesian linear regression}{7}{subsection.2.1.1}}
\newlabel{linearmodel}{{2.1}{7}{Standard Bayesian linear regression}{equation.2.1.1}{}}
\newlabel{wprior}{{2.3}{7}{Standard Bayesian linear regression}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kernel functions in feature space}{8}{subsection.2.1.2}}
\newlabel{torewrite}{{2.8}{8}{Kernel functions in feature space}{equation.2.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}A function space view for Gaussian Processes}{8}{section.2.2}}
\newlabel{sqexp}{{2.17}{9}{A function space view for Gaussian Processes}{equation.2.2.17}{}}
\newlabel{fprior}{{2.18}{9}{A function space view for Gaussian Processes}{equation.2.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Three sampled Gaussian Process priors using the Squared Exponential kernel.\relax }}{10}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:drawPrior}{{2.1}{10}{Three sampled Gaussian Process priors using the Squared Exponential kernel.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Prediction using a Gaussian Process prior}{11}{section.2.3}}
\newlabel{pred}{{2.3}{11}{Prediction using a Gaussian Process prior}{section.2.3}{}}
\newlabel{conditioning}{{1}{11}{Prediction using a Gaussian Process prior}{mydef.1}{}}
\newlabel{hierarchical}{{2.27}{12}{Prediction using a Gaussian Process prior}{equation.2.3.27}{}}
\newlabel{hierarchical2}{{2.28}{12}{Prediction using a Gaussian Process prior}{equation.2.3.28}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gaussian regressor pseudo-code.\relax }}{12}{algorithm.1}}
\newlabel{alg}{{1}{12}{Gaussian regressor pseudo-code.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}A toy example of Gaussian Process regression}{13}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Picking a winner}{14}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A fitted Gaussian Process regressor to samples of the sine function.\relax }}{15}{figure.caption.3}}
\newlabel{fig:GPsine}{{2.2}{15}{A fitted Gaussian Process regressor to samples of the sine function.\relax }{figure.caption.3}{}}
\newlabel{risk}{{2.32}{16}{Picking a winner}{equation.2.3.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}On covariance functions}{16}{section.2.4}}
\newlabel{covariancefunc}{{2.4}{16}{On covariance functions}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}A zoo of covariance functions}{18}{subsection.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Behaviour of different stationary covariance functions with the default parameters in pyGPGO.\relax }}{19}{figure.caption.4}}
\newlabel{fig:zoo}{{2.3}{19}{Behaviour of different stationary covariance functions with the default parameters in pyGPGO.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Hyperparameter optimization}{20}{section.2.5}}
\newlabel{hyper}{{2.5}{20}{Hyperparameter optimization}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Type II Maximum Likelihood}{20}{subsection.2.5.1}}
\newlabel{posteriorparam}{{2.38}{20}{Type II Maximum Likelihood}{equation.2.5.38}{}}
\newlabel{posteriorhyper}{{2.39}{20}{Type II Maximum Likelihood}{equation.2.5.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{Another toy example: Optimizing the characteristic length-scale}{21}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Cross validation}{22}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Further theoretical aspects}{22}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Gaussian processes as linear smoothers}{22}{subsection.2.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$.\relax }}{23}{figure.caption.6}}
\newlabel{fig:logmarginal}{{2.4}{23}{Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Explicit basis functions}{23}{subsection.2.6.2}}
\newlabel{gbasis}{{2.48}{24}{Explicit basis functions}{equation.2.6.48}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian optimization}{25}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries}{25}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The bayesian optimization framework}{25}{section.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Bayesian optimization framework.\relax }}{26}{algorithm.2}}
\newlabel{bayesopt}{{2}{26}{Bayesian optimization framework.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}On acquisition functions}{26}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Improvement-based policies}{26}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimistic policies}{27}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Information-based policies}{27}{subsection.3.3.3}}
\newlabel{acqes}{{3.10}{27}{Information-based policies}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Acquisition function portfolios}{28}{subsection.3.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Visualizing the behaviour of an acquisition function}{28}{subsection.3.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Why does Bayesian Optimization work?}{29}{subsection.3.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example.\relax }}{30}{figure.caption.7}}
\newlabel{fig:acqzoo}{{3.1}{30}{Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A visual explanation on why Bayesian optimization is efficient at exploring the space. It ignores all the input space where the UPB is lower than the point with maximum LCB.\relax }}{31}{figure.caption.8}}
\newlabel{fig:explanation}{{3.2}{31}{A visual explanation on why Bayesian optimization is efficient at exploring the space. It ignores all the input space where the UPB is lower than the point with maximum LCB.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Role of GP hyperparameters in optimization}{32}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Optimizing the acquisition function}{32}{section.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Computational costs}{33}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Approximations to the analytical GP. Alternative surrogates.}{33}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Parallelization}{34}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Step-by-step examples}{35}{section.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Optimizing (yet-again) the sine function}{35}{subsection.3.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Optimizing the Rossenbrock function}{35}{subsection.3.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Six complete optimization epochs in the Bayesian Optimization framework for the target sine function in $x\in [0, 2\pi ]$.\relax }}{36}{figure.caption.9}}
\newlabel{fig:1dstep}{{3.3}{36}{Six complete optimization epochs in the Bayesian Optimization framework for the target sine function in $x\in [0, 2\pi ]$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A 2D representation of the Rossenbrock function for $x, y \in [-1, 1]$.\relax }}{37}{figure.caption.10}}
\newlabel{fig:rosen}{{3.4}{37}{A 2D representation of the Rossenbrock function for $x, y \in [-1, 1]$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Six complete optimization epochs in the Bayesian Optimization framework for the target Rossenbrock 2D function.\relax }}{38}{figure.caption.11}}
\newlabel{fig:2dstep}{{3.5}{38}{Six complete optimization epochs in the Bayesian Optimization framework for the target Rossenbrock 2D function.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{39}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef\minted@oldcachelist{,
  4BC65608FD2763E7C231A03E58CF90C99D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  85B43FECD3A9742F76E3BCC90D3C48A79D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  CBAFD97C67C91ADDAEA34D3DAF0D40079D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  990BF002D0D2280EAC881D74A601ABB29D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  71F3DE110298F09031B3F38D0484E76D9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  1E29121FDD465C160E40FEBB4C5ACF719D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  21283FDE4A89D22A1CC5774B4CB55E0F9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  4114330659FF80695AA0581023CABCDE9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  01846CE0AB08FBB5370F551BDD8700E49D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  778010B1D483D75C07C76F2DA0721C6B9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  69039E575068EBA51D5DDA94862D7B089D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  C3FEF1F24A4C8C42E29EB147312F79BA9D188FF14B5B5A34504F2B9625FD44DE.pygtex}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}pyGPGO: A simple Python Package for Bayesian Optimization}{41}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
