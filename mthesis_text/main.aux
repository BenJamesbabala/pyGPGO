\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Organization of this work}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}How do I read this?}{6}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Gaussian Process regression}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A weight space view for Gaussian Processes}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Standard Bayesian linear regression}{7}{subsection.2.1.1}}
\newlabel{linearmodel}{{2.1}{7}{Standard Bayesian linear regression}{equation.2.1.1}{}}
\newlabel{wprior}{{2.3}{7}{Standard Bayesian linear regression}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kernel functions in feature space}{8}{subsection.2.1.2}}
\newlabel{torewrite}{{2.8}{8}{Kernel functions in feature space}{equation.2.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}A function space view for Gaussian Processes}{8}{section.2.2}}
\newlabel{sqexp}{{2.17}{9}{A function space view for Gaussian Processes}{equation.2.2.17}{}}
\newlabel{fprior}{{2.18}{9}{A function space view for Gaussian Processes}{equation.2.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Three sampled Gaussian Process priors using the Squared Exponential kernel.}}{10}{figure.2.1}}
\newlabel{fig:drawPrior}{{2.1}{10}{Three sampled Gaussian Process priors using the Squared Exponential kernel}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Prediction using a Gaussian Process prior}{10}{section.2.3}}
\newlabel{pred}{{2.3}{10}{Prediction using a Gaussian Process prior}{section.2.3}{}}
\newlabel{conditioning}{{1}{10}{Prediction using a Gaussian Process prior}{mydef.1}{}}
\newlabel{hierarchical}{{2.27}{11}{Prediction using a Gaussian Process prior}{equation.2.3.27}{}}
\newlabel{hierarchical2}{{2.28}{11}{Prediction using a Gaussian Process prior}{equation.2.3.28}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gaussian regressor pseudo-code.}}{12}{algorithm.1}}
\newlabel{alg}{{1}{12}{Prediction using a Gaussian Process prior}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}A toy example of Gaussian Process regression}{13}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}On covariance functions}{14}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A fitted Gaussian Process regressor to samples of the sine function.}}{15}{figure.2.2}}
\newlabel{fig:GPsine}{{2.2}{15}{A fitted Gaussian Process regressor to samples of the sine function}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}A zoo of covariance functions}{17}{subsection.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Behaviour of different stationary covariance functions with the default parameters in pyGPGO.}}{18}{figure.2.3}}
\newlabel{fig:zoo}{{2.3}{18}{Behaviour of different stationary covariance functions with the default parameters in pyGPGO}{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Hyperparameter optimization}{18}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Type II Maximum Likelihood}{18}{subsection.2.5.1}}
\newlabel{posteriorparam}{{2.36}{18}{Type II Maximum Likelihood}{equation.2.5.36}{}}
\newlabel{posteriorhyper}{{2.37}{19}{Type II Maximum Likelihood}{equation.2.5.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Another toy example: Optimizing the characteristic length-scale}{20}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Cross validation}{20}{subsection.2.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$.}}{21}{figure.2.4}}
\newlabel{fig:logmarginal}{{2.4}{21}{Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$}{figure.2.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian optimization}{23}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries}{23}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The bayesian optimization framework}{23}{section.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Bayesian optimization framework.}}{24}{algorithm.2}}
\newlabel{bayesopt}{{2}{24}{The bayesian optimization framework}{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}On acquisition functions}{24}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Improvement-based policies}{24}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimistic policies}{25}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Information-based policies}{25}{subsection.3.3.3}}
\newlabel{acqes}{{3.10}{25}{Information-based policies}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Example: visualizing the behaviour of an acquisition function}{25}{subsection.3.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example.}}{27}{figure.3.1}}
\newlabel{fig:acqzoo}{{3.1}{27}{Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example}{figure.3.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{29}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef\minted@oldcachelist{,
  4BC65608FD2763E7C231A03E58CF90C99D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  85B43FECD3A9742F76E3BCC90D3C48A79D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  CBAFD97C67C91ADDAEA34D3DAF0D40079D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  990BF002D0D2280EAC881D74A601ABB29D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  71F3DE110298F09031B3F38D0484E76D9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  1E29121FDD465C160E40FEBB4C5ACF719D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  21283FDE4A89D22A1CC5774B4CB55E0F9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  4114330659FF80695AA0581023CABCDE9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  01846CE0AB08FBB5370F551BDD8700E49D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  778010B1D483D75C07C76F2DA0721C6B9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  69039E575068EBA51D5DDA94862D7B089D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  C3FEF1F24A4C8C42E29EB147312F79BA9D188FF14B5B5A34504F2B9625FD44DE.pygtex}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}pyGPGO: A simple Python Package for Bayesian Optimization}{31}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
