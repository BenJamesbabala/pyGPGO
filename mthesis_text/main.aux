\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Organization of this work}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{5}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Organization of the thesis}{6}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Gaussian Process regression}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A weight space view for Gaussian Processes}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Standard Bayesian linear regression}{7}{subsection.2.1.1}}
\newlabel{linearmodel}{{2.1}{7}{Standard Bayesian linear regression}{equation.2.1.1}{}}
\newlabel{wprior}{{2.3}{7}{Standard Bayesian linear regression}{equation.2.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kernel functions in feature space}{8}{subsection.2.1.2}}
\newlabel{torewrite}{{2.8}{8}{Kernel functions in feature space}{equation.2.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}A function space view for Gaussian Processes}{8}{section.2.2}}
\newlabel{sqexp}{{2.17}{9}{A function space view for Gaussian Processes}{equation.2.2.17}{}}
\newlabel{fprior}{{2.18}{9}{A function space view for Gaussian Processes}{equation.2.2.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Prediction using a Gaussian Process prior}{9}{section.2.3}}
\newlabel{pred}{{2.3}{9}{Prediction using a Gaussian Process prior}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Three sampled Gaussian Process priors using the Squared Exponential kernel.\relax }}{10}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:drawPrior}{{2.1}{10}{Three sampled Gaussian Process priors using the Squared Exponential kernel.\relax }{figure.caption.2}{}}
\newlabel{conditioning}{{1}{11}{Prediction using a Gaussian Process prior}{mydef.1}{}}
\newlabel{hierarchical}{{2.27}{11}{Prediction using a Gaussian Process prior}{equation.2.3.27}{}}
\newlabel{hierarchical2}{{2.28}{11}{Prediction using a Gaussian Process prior}{equation.2.3.28}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gaussian regressor pseudo-code.\relax }}{12}{algorithm.1}}
\newlabel{alg}{{1}{12}{Gaussian regressor pseudo-code.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}A toy example of Gaussian Process regression}{12}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Picking a winner}{12}{subsection.2.3.2}}
\newlabel{risk}{{2.32}{12}{Picking a winner}{equation.2.3.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A fitted Gaussian Process regressor to samples of the sine function.\relax }}{13}{figure.caption.3}}
\newlabel{fig:GPsine}{{2.2}{13}{A fitted Gaussian Process regressor to samples of the sine function.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}On covariance functions}{14}{section.2.4}}
\newlabel{covariancefunc}{{2.4}{14}{On covariance functions}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Behaviour of different stationary covariance functions with the default parameters in pyGPGO.\relax }}{15}{figure.caption.4}}
\newlabel{fig:zoo}{{2.3}{15}{Behaviour of different stationary covariance functions with the default parameters in pyGPGO.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Visualizing different covariance functions}{15}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Hyperparameter optimization}{16}{section.2.5}}
\newlabel{hyper}{{2.5}{16}{Hyperparameter optimization}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Type II Maximum Likelihood}{16}{subsection.2.5.1}}
\newlabel{empbayes}{{2.5.1}{16}{Type II Maximum Likelihood}{subsection.2.5.1}{}}
\newlabel{posteriorparam}{{2.39}{16}{Type II Maximum Likelihood}{equation.2.5.39}{}}
\newlabel{posteriorhyper}{{2.40}{16}{Type II Maximum Likelihood}{equation.2.5.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Another toy example: Optimizing the characteristic length-scale}{16}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$.\relax }}{17}{figure.caption.6}}
\newlabel{fig:logmarginal}{{2.4}{17}{Log-marginal likelihood and its gradient w.r.t to the characteristic length-scale. Notice there seems to be an optimal point at around $l = 1.4$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Cross validation}{17}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Further theoretical aspects}{17}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Gaussian processes as linear smoothers}{17}{subsection.2.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Explicit basis functions}{18}{subsection.2.6.2}}
\newlabel{gbasis}{{2.49}{18}{Explicit basis functions}{equation.2.6.49}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian optimization}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries}{21}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The bayesian optimization framework}{21}{section.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Bayesian optimization framework.\relax }}{22}{algorithm.2}}
\newlabel{bayesopt}{{2}{22}{Bayesian optimization framework.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}On acquisition functions}{22}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Improvement-based policies}{22}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Optimistic policies}{23}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Information-based policies}{23}{subsection.3.3.3}}
\newlabel{acqes}{{3.11}{23}{Information-based policies}{equation.3.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Acquisition function portfolios}{24}{subsection.3.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Visualizing the behaviour of an acquisition function}{24}{subsection.3.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Why does Bayesian Optimization work?}{24}{subsection.3.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example.\relax }}{25}{figure.caption.7}}
\newlabel{fig:acqzoo}{{3.1}{25}{Acquisition function behaviour for Expected Improvement, Probability of Improvement, GP-UCB ($\beta = .5$) and GP-UCB($\beta = 1.5$) in the sine function example.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A visual explanation on why Bayesian optimization is efficient at exploring the space. It ignores all the input space where the UPB is lower than the point with maximum LCB.\relax }}{26}{figure.caption.8}}
\newlabel{fig:explanation}{{3.2}{26}{A visual explanation on why Bayesian optimization is efficient at exploring the space. It ignores all the input space where the UPB is lower than the point with maximum LCB.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Role of GP hyperparameters in optimization}{26}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Optimizing the acquisition function}{27}{section.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Computational costs}{28}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Approximations to the analytical GP. Alternative surrogates.}{28}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Parallelization}{29}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Step-by-step examples}{30}{section.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Optimizing the sine function}{30}{subsection.3.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Optimizing the Rastrigin function}{30}{subsection.3.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Six complete optimization epochs in the Bayesian Optimization framework for the target sine function in $x\in [0, 2\pi ]$.\relax }}{31}{figure.caption.9}}
\newlabel{fig:1dstep}{{3.3}{31}{Six complete optimization epochs in the Bayesian Optimization framework for the target sine function in $x\in [0, 2\pi ]$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A 2D representation of the Rossenbrock function for $x, y \in [-1, 1]$.\relax }}{32}{figure.caption.10}}
\newlabel{fig:rosen}{{3.4}{32}{A 2D representation of the Rossenbrock function for $x, y \in [-1, 1]$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Six complete optimization epochs in the Bayesian Optimization framework for the target Rossenbrock 2D function.\relax }}{33}{figure.caption.11}}
\newlabel{fig:2dstep}{{3.5}{33}{Six complete optimization epochs in the Bayesian Optimization framework for the target Rossenbrock 2D function.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{35}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Benchmarking rules}{35}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Other strategies for hyper-parameter optimization}{35}{subsection.4.1.1}}
\newlabel{sametro}{{4.1}{36}{Other strategies for hyper-parameter optimization}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Evaluation metrics}{36}{subsection.4.1.2}}
\newlabel{evaluation}{{4.1.2}{36}{Evaluation metrics}{subsection.4.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Bayesian optimization setup}{37}{subsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Machine-learning models used}{37}{subsection.4.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The binding affinity dataset}{37}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Description of the problem}{37}{subsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Small illustration of the docking procedure. Ligand poses generating by a docking program are ranked according to a given scoring function, hopefully resulting in a valid complex.\relax }}{38}{figure.caption.12}}
\newlabel{fig:docking}{{4.1}{38}{Small illustration of the docking procedure. Ligand poses generating by a docking program are ranked according to a given scoring function, hopefully resulting in a valid complex.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Description of the dataset}{38}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Experiments}{39}{subsection.4.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces SVM results for the binding affinity dataset.\relax }}{40}{figure.caption.13}}
\newlabel{fig:affsvm}{{4.2}{40}{SVM results for the binding affinity dataset.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces MLP results for the binding affinity dataset.\relax }}{40}{figure.caption.14}}
\newlabel{fig:affmlp}{{4.3}{40}{MLP results for the binding affinity dataset.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces AdaBoost results for the binding affinity dataset.\relax }}{41}{figure.caption.15}}
\newlabel{fig:affada}{{4.4}{41}{AdaBoost results for the binding affinity dataset.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Gradient Boosting Machine results for the binding affinity dataset.\relax }}{41}{figure.caption.16}}
\newlabel{fig:affgbm}{{4.5}{41}{Gradient Boosting Machine results for the binding affinity dataset.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The LSVT companion dataset}{42}{section.4.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}pyGPGO: A simple Python Package for Bayesian Optimization}{43}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Installation}{43}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Usage}{43}{section.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Examples}{43}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Features}{43}{section.5.4}}
\newlabel{features}{{5.4}{43}{Features}{section.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Comparison with existing software}{43}{section.5.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Future work}{43}{section.5.6}}
\newlabel{drawGP}{{5.6}{45}{drawGP.py}{section*.19}{}}
\newlabel{sineGP}{{5.6}{45}{sineGP.py}{section*.20}{}}
\newlabel{covzoo}{{5.6}{46}{covzoo.py}{section*.21}{}}
\newlabel{hyperopt}{{5.6}{47}{hyperopt.py}{section*.22}{}}
\newlabel{bayoptwork}{{5.6}{49}{bayoptwork.py}{section*.24}{}}
\newlabel{sineopt}{{5.6}{50}{sineopt.py}{section*.25}{}}
\newlabel{rastriginopt}{{5.6}{51}{rastriginopt.py}{section*.26}{}}
\gdef\minted@oldcachelist{,
  4BC65608FD2763E7C231A03E58CF90C99D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  9F893418F765481B17A6CE89FA4C45BB9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  01846CE0AB08FBB5370F551BDD8700E49D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  69039E575068EBA51D5DDA94862D7B089D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  C3FEF1F24A4C8C42E29EB147312F79BA9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  B5A79309824013F4C4719760E078636C9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  FB8DA0D5E2C7281D9638DECE993988EC9D188FF14B5B5A34504F2B9625FD44DE.pygtex,
  E3BFAF37CEF84C55BFC527FF6725FA999D188FF14B5B5A34504F2B9625FD44DE.pygtex}
