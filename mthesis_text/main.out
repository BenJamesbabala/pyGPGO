\BOOKMARK [0][-]{chapter.1}{Organization of this work}{}% 1
\BOOKMARK [1][-]{section.1.1}{Introduction}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Organization of the thesis}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Gaussian Process regression}{}% 4
\BOOKMARK [1][-]{section.2.1}{A function space view for Gaussian Processes}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{A weight space view for Gaussian Processes}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Standard Bayesian linear regression}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Kernel functions in feature space}{section.2.2}% 8
\BOOKMARK [1][-]{section.2.3}{Prediction using a Gaussian Process prior}{chapter.2}% 9
\BOOKMARK [2][-]{subsection.2.3.1}{A toy example of Gaussian Process regression}{section.2.3}% 10
\BOOKMARK [2][-]{subsection.2.3.2}{Picking a winner}{section.2.3}% 11
\BOOKMARK [1][-]{section.2.4}{On covariance functions}{chapter.2}% 12
\BOOKMARK [2][-]{subsection.2.4.1}{Visualizing different covariance functions}{section.2.4}% 13
\BOOKMARK [1][-]{section.2.5}{Hyperparameter optimization}{chapter.2}% 14
\BOOKMARK [2][-]{subsection.2.5.1}{Type II Maximum Likelihood}{section.2.5}% 15
\BOOKMARK [2][-]{subsection.2.5.2}{Cross validation}{section.2.5}% 16
\BOOKMARK [1][-]{section.2.6}{Further theoretical aspects}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.6.1}{Gaussian processes as linear smoothers}{section.2.6}% 18
\BOOKMARK [2][-]{subsection.2.6.2}{Explicit basis functions}{section.2.6}% 19
\BOOKMARK [2][-]{subsection.2.6.3}{Marginalizing over hyperparameters}{section.2.6}% 20
\BOOKMARK [0][-]{chapter.3}{Bayesian optimization}{}% 21
\BOOKMARK [1][-]{section.3.1}{Preliminaries}{chapter.3}% 22
\BOOKMARK [1][-]{section.3.2}{The bayesian optimization framework}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.3}{On acquisition functions}{chapter.3}% 24
\BOOKMARK [2][-]{subsection.3.3.1}{Improvement-based policies}{section.3.3}% 25
\BOOKMARK [2][-]{subsection.3.3.2}{Optimistic policies}{section.3.3}% 26
\BOOKMARK [2][-]{subsection.3.3.3}{Information-based policies}{section.3.3}% 27
\BOOKMARK [2][-]{subsection.3.3.4}{Acquisition function portfolios}{section.3.3}% 28
\BOOKMARK [2][-]{subsection.3.3.5}{Visualizing the behaviour of an acquisition function}{section.3.3}% 29
\BOOKMARK [2][-]{subsection.3.3.6}{Why does Bayesian Optimization work?}{section.3.3}% 30
\BOOKMARK [1][-]{section.3.4}{Role of GP hyperparameters in optimization}{chapter.3}% 31
\BOOKMARK [1][-]{section.3.5}{Optimizing the acquisition function}{chapter.3}% 32
\BOOKMARK [1][-]{section.3.6}{Computational costs}{chapter.3}% 33
\BOOKMARK [2][-]{subsection.3.6.1}{Approximations to the analytical GP. Alternative surrogates.}{section.3.6}% 34
\BOOKMARK [2][-]{subsection.3.6.2}{Parallelization}{section.3.6}% 35
\BOOKMARK [1][-]{section.3.7}{Step-by-step examples}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.7.1}{Optimizing the sine function}{section.3.7}% 37
\BOOKMARK [2][-]{subsection.3.7.2}{Optimizing the Rastrigin function}{section.3.7}% 38
\BOOKMARK [0][-]{chapter.4}{Experiments}{}% 39
\BOOKMARK [1][-]{section.4.1}{Benchmarking rules}{chapter.4}% 40
\BOOKMARK [2][-]{subsection.4.1.1}{Other strategies for hyper-parameter optimization}{section.4.1}% 41
\BOOKMARK [2][-]{subsection.4.1.2}{Evaluation metrics}{section.4.1}% 42
\BOOKMARK [2][-]{subsection.4.1.3}{Bayesian optimization setup}{section.4.1}% 43
\BOOKMARK [2][-]{subsection.4.1.4}{Machine-learning models used}{section.4.1}% 44
\BOOKMARK [1][-]{section.4.2}{The binding affinity dataset}{chapter.4}% 45
\BOOKMARK [2][-]{subsection.4.2.1}{Description of the problem}{section.4.2}% 46
\BOOKMARK [2][-]{subsection.4.2.2}{Description of the dataset}{section.4.2}% 47
\BOOKMARK [2][-]{subsection.4.2.3}{Experiments}{section.4.2}% 48
\BOOKMARK [1][-]{section.4.3}{The protein-protein interface prediction dataset}{chapter.4}% 49
\BOOKMARK [2][-]{subsection.4.3.1}{Description of the problem}{section.4.3}% 50
\BOOKMARK [2][-]{subsection.4.3.2}{Description of the dataset}{section.4.3}% 51
\BOOKMARK [2][-]{subsection.4.3.3}{Experiments}{section.4.3}% 52
\BOOKMARK [1][-]{section.4.4}{Other datasets}{chapter.4}% 53
\BOOKMARK [0][-]{chapter.5}{pyGPGO: A simple Python Package for Bayesian Optimization}{}% 54
\BOOKMARK [1][-]{section.5.1}{Installation}{chapter.5}% 55
\BOOKMARK [1][-]{section.5.2}{Usage}{chapter.5}% 56
\BOOKMARK [2][-]{subsection.5.2.1}{A minimal example}{section.5.2}% 57
\BOOKMARK [1][-]{section.5.3}{Examples}{chapter.5}% 58
\BOOKMARK [2][-]{subsection.5.3.1}{Gaussian Process regression using the GPRegressor module.}{section.5.3}% 59
\BOOKMARK [2][-]{subsection.5.3.2}{Optimizing parameters of a machine-learning model using the GPGO module.}{section.5.3}% 60
\BOOKMARK [1][-]{section.5.4}{Features}{chapter.5}% 61
\BOOKMARK [1][-]{section.5.5}{Comparison with existing software}{chapter.5}% 62
\BOOKMARK [1][-]{section.5.6}{Future work}{chapter.5}% 63
