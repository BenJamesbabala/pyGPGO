@misc{Neal2003,
abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal " slice " defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such " slice sampling " methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by " overrelaxation, " and for multivariate slice sampling by " reflection " from the edges of the slice.},
archivePrefix = {arXiv},
arxivId = {1003.3201v1},
author = {Neal, Radford M.},
booktitle = {Annals of Statistics},
doi = {10.1214/aos/1056562461},
eprint = {1003.3201v1},
isbn = {00905364},
issn = {00905364},
number = {3},
pages = {758--767},
title = {{Slice sampling: Rejoinder}},
volume = {31},
year = {2003}
}

@article{Murray2009,
abstract = {Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.},
archivePrefix = {arXiv},
arxivId = {1001.0175},
author = {Murray, Iain and Adams, Ryan Prescott RP and MacKay, DJC David J. C.},
eprint = {1001.0175},
issn = {15324435},
journal = {arXiv preprint arXiv:1001.0175},
number = {2},
pages = {8},
title = {{Elliptical slice sampling}},
url = {http://www.jmlr.org/proceedings/papers/v9/murray10a/murray10a.pdf$\backslash$nhttp://arxiv.org/abs/1001.0175},
year = {2009}
}


@article{Murray2010,
abstract = {The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.},
archivePrefix = {arXiv},
arxivId = {1006.0868},
author = {Murray, Iain and Adams, Ryan Prescott},
eprint = {1006.0868},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing {\ldots}},
number = {1},
pages = {9},
title = {{Slice sampling covariance hyperparameters of latent Gaussian models}},
url = {http://papers.nips.cc/paper/4114-slice-sampling-covariance-hyperparameters-of-latent-gaussian-models{\%}5Cnhttp://arxiv.org/abs/1006.0868},
volume = {2},
year = {2010}
}

@article{Bolstad2007,
abstract = {This chapter contains sections titled:$\backslash$n$\backslash$n* Least Squares Regression * Exponential Growth Model * Simple Linear$\backslash$nRegression Assumptions * Bayes' Theorem for the Regression Model$\backslash$n* Predictive Distribution for Future Observation * Exercises * Computer$\backslash$nExercises},
author = {Bolstad, William M},
doi = {10.1002/9780470181188.ch14},
isbn = {9780470181188},
journal = {Introduction to Bayesian Statistics},
keywords = {Bayesian inference,exponential growth model,joint prior,least squares regression,simple linear regression},
pages = {267--295},
title = {{Bayesian Inference for Simple Linear Regression}},
url = {http://dx.doi.org/10.1002/9780470181188.ch14},
year = {2007}
}

@misc{Hofmann2008,
abstract = {We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0701907v3},
author = {Hofmann, Thomas and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J.},
booktitle = {Annals of Statistics},
doi = {10.1214/009053607000000677},
eprint = {0701907v3},
isbn = {0090-5364},
issn = {00905364},
keywords = {Graphical models,Machine learning,Reproducing kernels,Support vector machines},
number = {3},
pages = {1171--1220},
primaryClass = {arXiv:math},
title = {{Kernel methods in machine learning}},
volume = {36},
year = {2008}
}

@article{Wackernagel1995,
abstract = {Geostatistics offers a variety of models, methods and techniques for the analysis, estimation and display of multivariate data distributed in space or time. The book presents a brief review of statistical concepts, a detailed introduction to linear geostatistics, and an account of three basic methods of multivariate analysis. It contains an advanced presentation of linear models for multivariate spatial or temporal data, including the bilinear model of coregionalization, and an introduction to non-stationary geostatistics with a special focus on the external drift method. The 30 chapters are presented in five parts: preliminaries, geostatistics, multivariate analysis, multivariate geostatistics, non-stationary geostatistics. -from Publisher},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wackernagel, H.},
doi = {10.1016/S0098-3004(97)87526-7},
eprint = {arXiv:1011.1669v3},
isbn = {3540441425},
issn = {3540601279 (ISBN)},
journal = {Multivariate geostatistics: an introduction with applications},
pmid = {4420},
title = {{Multivariate geostatistics: an introduction with applications}},
year = {1995}
}

@book{Rasmussen2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Rasmussen, Carl E. and Williams, Christopher K. I.},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
isbn = {026218253X},
issn = {0129-0657},
keywords = {2006,c,c 2006 massachusetts institute,e,gaussian processes for machine,gaussianprocess,gpml,i,isbn 026218253x,k,learning,of technology,org,rasmussen,the mit press,williams,www},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
volume = {14},
year = {2004}
}

@book{Arfken2005,
abstract = {Through six editions now, Mathematical Methods for Physicists has provided all the mathematical methods that aspirings scientists and engineers are likely to encounter as students and beginning researchers. More than enough material is included for a two-semester undergraduate or graduate course. The book is advanced in the sense that mathematical relations are almost always proven, in addition to being illustrated in terms of examples. These proofs are not what a mathematician would regard as rigorous, but sketch the ideas and emphasize the relations that are essential to the study of physics and related fields. This approach incorporates theorems that are usually not cited under the most general assumptions, but are tailored to the more restricted applications required by physics. For example, Stokes' theorem is usually applied by a physicist to a surface with the tacit understanding that it be simply connected. Such assumptions have been made more explicit.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Arfken, George},
booktitle = {American Journal of Physics},
doi = {10.1119/1.1988084},
eprint = {arXiv:1011.1669v3},
isbn = {0120598760},
issn = {00029505},
number = {4},
pages = {642},
pmid = {5412156},
title = {{Mathematical Methods for Physicists 6th}},
volume = {40},
year = {2005}
}

@article{Minasny2005,
abstract = {The variogram is important in pedometrics for describing and quantifying soil spatial variability. Therefore, it is essential to have a model that can describe various spatial processes and to use appropriate techniques for estimating its parameters. The Mat{\'{e}}rn model is a generalization of several theoretical variogram models that incorporates a smoothness parameter. We show the flexibility of the Mat{\'{e}}rn model using simulation and apply the Mat{\'{e}}rn model to some soil data in Australia. Parameters of the Mat{\'{e}}rn model were determined by restricted maximum likelihood (REML), and weighted nonlinear least-squares (WNLS) on the empirical variogram. The Mat{\'{e}}rn model is shown to be flexible and can be used to describe many isotropic spatial soil processes. The REML method fits the local spatial process correctly, however the drawback is the lengthy computation. Meanwhile WNLS fits only the shape of the calculated empirical variogram, and parameters estimated from WNLS can be misleading. From this study, the smoothness parameter of soil data from point measurement appears to be in the range of 0.25-0.50 and can be considered to be a rough spatial process. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Minasny, Budiman and McBratney, Alex B.},
doi = {10.1016/j.geoderma.2005.04.003},
isbn = {00167061},
issn = {00167061},
journal = {Geoderma},
pages = {192--207},
title = {{The matern function as a general model for soil variograms}},
volume = {128},
year = {2005}
}

@book{Rossi2006,
abstract = {Bayesian methods have become widespread in marketing literature. We review the essence of the Bayesian approach and explain why it is particularly useful for marketing problems. While the appeal of the Bayesian approach has long been noted by researchers, recent ... $\backslash$n},
author = {Rossi, Peter E. and Allenby, Greg M. and McCulloch, Robert},
booktitle = {Bayesian Statistics and Marketing},
doi = {10.1002/0470863692},
isbn = {9780470863695},
issn = {0732-2399},
pages = {1--348},
pmid = {212261985},
title = {{Bayesian Statistics and Marketing}},
year = {2006}
}

@article{BUJA1989,
abstract = {We study linear smoothers and their use in building nonparametric regression models. In the first part of this paper we examine certain aspects of linear smoothers for scatterplots; examples of these are the running-mean and running-line, kernel and cubic spline smoothers. The eigenvalue and singular value decompositions of the corresponding smoother matrix are used to describe qualitatively a smoother, and several other topics such as the number of degrees of freedom of a smoother are discussed. In the second part of the paper we describe how linear smoothers can be used to estimate the additive model, a powerful nonparametric regression model, using the " back- fitting algorithm." We show that backfitting is the Gauss-Seidel iterative method for solving a set of normal equations associated with the additive model. We provide conditions for consistency and nondegeneracy and prove convergence for the backfitting and related algorithms for a class of smoothers that includes cubic spline smoothers},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Buja, A. and Hastie, T. and Tibshirani, R.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {The Annals of Statistic},
keywords = {icle},
number = {2},
pages = {453--555},
pmid = {25246403},
title = {{Linear smoothers and additive models}},
volume = {17},
year = {1989}
}

@book{Wahba1991,
abstract = {This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a number of},
author = {Wahba, Grace},
booktitle = {SIAM Review},
doi = {10.1137/1033124},
isbn = {0-89871-244-0},
issn = {0036-1445},
number = {3},
pages = {502--502},
title = {{Spline Models for Observational Data}},
volume = {33},
year = {1991}
}





